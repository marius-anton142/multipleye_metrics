# -*- coding: utf-8 -*-
"""metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FhBV_PfHjc4KKeWk9zGOb6idJ0ZH5qKl
"""

import re
import spacy

def pronouns_per_sentence(stimuli, nlp):
    values = []
    for stim in stimuli:
        text = " ".join(stim["pages"])
        doc = nlp(text)

        for sent in doc.sents:
            count = 0
            for token in sent:
                if token.pos_ == "PRON":
                    count += 1
            values.append(count)

    if values:
        avg = sum(values) / len(values)
    else:
        avg = 0
    return values, avg

def punctuation_per_sentence(stimuli, nlp):
    values = []
    for stim in stimuli:
        text = " ".join(stim["pages"])
        doc = nlp(text)

        for sent in doc.sents:
            count = 0
            for token in sent:
                if token.is_punct:
                    count += 1
            values.append(count)

    if values:
        avg = sum(values) / len(values)
    else:
        avg = 0
    return values, avg

def fertility(stimuli, nlp, tokenizer):
    values = []
    for stim in stimuli:
        text = " ".join(stim["pages"])
        doc = nlp(text)

        for token in doc:
            if token.is_alpha:
                llm_tokens = tokenizer.encode(token.text, add_special_tokens=False)
                count = len(llm_tokens)
                values.append(count)
    if values:
        avg = sum(values) / len(values)
    else:
        avg = 0
    return values, avg

def compute_ttr(stimuli, nlp):
    import re
    import pandas as pd

    def preprocess(text):
        text = text.lower()
        text = re.sub(r"[^a-z0-9\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip() #replace multiple spaces
        return text

    def ttr_from_text(text):
        doc = nlp(text)
        tokens = [t.text for t in doc if t.is_alpha]
        if not tokens:
            return 0.0, 0, 0
        types = set(tokens)
        return len(types) / len(tokens), len(tokens), len(types)

    rows = []
    page_ttr_values = []

    for stim in stimuli:
        sid = stim["stimulus_id"]
        sname = stim["stimulus_name"]

        full = preprocess(" ".join(stim["pages"]))
        ttr_full, n_tok_full, n_types_full = ttr_from_text(full)
        rows.append({
            "stimulus_id": sid,
            "stimulus_name": sname,
            "scope": "full_text",
            "page": None,
            "num_tokens": n_tok_full,
            "num_types": n_types_full,
            "ttr": ttr_full
        })

        for i, page in enumerate(stim["pages"], start=1):
            ptext = preprocess(page)
            ttr_page, n_tok_page, n_types_page = ttr_from_text(ptext)
            rows.append({
                "stimulus_id": sid,
                "stimulus_name": sname,
                "scope": "page",
                "page": i,
                "num_tokens": n_tok_page,
                "num_types": n_types_page,
                "ttr": ttr_page
            })
            page_ttr_values.append(ttr_page)

    df = pd.DataFrame(rows).sort_values(["stimulus_id", "scope", "page"], ignore_index=True)
    avg_pages = sum(page_ttr_values) / len(page_ttr_values) if page_ttr_values else 0

    return df, page_ttr_values, avg_pages